'use client'

import { useState, useRef, useEffect, useCallback, memo } from 'react'
import MessageBubble from './MessageBubble'
import InputBar from './InputBar'
import LoadingStream from './LoadingStream'
import VoiceButton from './VoiceButton'
import AudioPlayer from './AudioPlayer'
import { Language } from '@/utils/languages'
import { apiClient } from '@/utils/api'
import { getUserLocation } from '@/utils/location'
import { useVoiceRecorder } from '@/hooks/useVoiceRecorder'
import { sanitizeText } from '@/utils/textUtils'
import { API_BASE } from '@/config/api'

export interface Message {
  id: string
  text: string
  sender: 'user' | 'agent'
  timestamp: Date
  /** true = char-by-char typing animation (text chat); false = instant render */
  streaming?: boolean
  /** 'streaming' while chunks arrive, 'final' when stream_end received */
  status?: 'streaming' | 'final'
}

interface ChatWindowProps {
  currentLanguage: Language
}

/**
 * ‚îÄ‚îÄ VOICE FLOW STATE MACHINE ‚îÄ‚îÄ
 *
 *  IDLE
 *    ‚Üí (user clicks mic)
 *  RECORDING
 *    ‚Üí (user clicks stop)
 *  PROCESSING              ‚Üê LoadingStream dots shown
 *    ‚Üí (transcript arrives)
 *  DISPLAYING_TRANSCRIPT   ‚Üê user message inserted, timestamp recorded
 *    ‚Üí (first stream_chunk)
 *  STREAMING_RESPONSE      ‚Üê single AI bubble updated in-place
 *    ‚Üí (stream_end)
 *  WAITING_AUDIO_DELAY     ‚Üê ‚â•1 s after transcript was shown
 *    ‚Üí (delay elapsed)
 *  PLAYING_AUDIO           ‚Üê AudioPlayer autoplays
 *    ‚Üí (playback ends / stop)
 *  IDLE
 */

export default function ChatWindow({ currentLanguage }: ChatWindowProps) {
  const [messages, setMessages] = useState<Message[]>([])
  const [isLoading, setIsLoading] = useState(false)
  const [audioUrl, setAudioUrl] = useState<string | null>(null)

  // ‚îÄ‚îÄ Refs for streaming dedup & audio timing ‚îÄ‚îÄ
  /** ID of the single AI bubble being built by stream_chunk messages */
  const streamingMsgIdRef = useRef<string | null>(null)
  /** Timestamp when the transcript (user message) was inserted */
  const transcriptShownAtRef = useRef<number | null>(null)
  /** Pending audio URL held until delay elapses */
  const pendingAudioUrlRef = useRef<string | null>(null)
  /** Timer for audio delay */
  const audioDelayTimerRef = useRef<ReturnType<typeof setTimeout> | null>(null)

  const messagesEndRef = useRef<HTMLDivElement>(null)
  const messagesContainerRef = useRef<HTMLDivElement>(null)
  /** Track if user has scrolled up (disable auto-scroll) */
  const isNearBottomRef = useRef(true)

  // ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ

  const clearAudioDelayTimer = useCallback(() => {
    if (audioDelayTimerRef.current) {
      clearTimeout(audioDelayTimerRef.current)
      audioDelayTimerRef.current = null
    }
  }, [])

  /**
   * Schedule audio playback so it starts ‚â• 1 000 ms after the transcript was shown.
   * If the delay has already elapsed by the time audio_url arrives, play immediately.
   */
  const scheduleAudioPlayback = useCallback(
    (url: string) => {
      clearAudioDelayTimer()
      const shownAt = transcriptShownAtRef.current ?? Date.now()
      const elapsed = Date.now() - shownAt
      const delay = Math.max(1000 - elapsed, 0)
      if (delay === 0) {
        setAudioUrl(url)
      } else {
        audioDelayTimerRef.current = setTimeout(() => {
          setAudioUrl(url)
        }, delay)
      }
    },
    [clearAudioDelayTimer]
  )

  // ‚îÄ‚îÄ Voice callbacks (stable refs wired to single-bubble logic) ‚îÄ‚îÄ

  /**
   * TRANSCRIPT ‚Äî insert user message & record timestamp.
   */
  const handleVoiceTranscript = useCallback((text: string, _language: string) => {
    const userMessage: Message = {
      id: Date.now().toString(),
      text: sanitizeText(text),
      sender: 'user',
      timestamp: new Date(),
    }
    transcriptShownAtRef.current = Date.now()
    setMessages((prev) => [...prev, userMessage])
  }, [])

  /**
   * STREAM_CHUNK ‚Äî create or update a SINGLE AI bubble.
   *
   * RULES:
   *   ‚Ä¢ Exactly ONE message bubble per response.
   *   ‚Ä¢ First chunk creates it (status='streaming').
   *   ‚Ä¢ Subsequent chunks update its text in-place via `updateMessage(id)`.
   *   ‚Ä¢ Never push a new message while streamingMsgIdRef is set.
   */
  const handleVoiceStreamChunk = useCallback((accumulatedText: string) => {
    const clean = sanitizeText(accumulatedText)
    if (!clean) return

    setMessages((prev) => {
      // ‚îÄ‚îÄ Try updating the existing streaming bubble ‚îÄ‚îÄ
      if (streamingMsgIdRef.current) {
        const idx = prev.findIndex((m) => m.id === streamingMsgIdRef.current)
        if (idx >= 0) {
          const updated = [...prev]
          updated[idx] = { ...updated[idx], text: clean }
          return updated
        }
        // If the message was somehow removed, fall through to create
      }

      // ‚îÄ‚îÄ First chunk: create the single streaming bubble ‚îÄ‚îÄ
      const id = `voice-ai-${Date.now()}`
      streamingMsgIdRef.current = id
      return [
        ...prev,
        {
          id,
          text: clean,
          sender: 'agent' as const,
          timestamp: new Date(),
          streaming: false, // no char-by-char animation for live voice chunks
          status: 'streaming' as const,
        },
      ]
    })
  }, [])

  /**
   * STREAM_END ‚Äî finalize the SAME bubble with the authoritative complete_response.
   *
   * RULES:
   *   ‚Ä¢ Replace the streaming bubble's text with `stream_end.complete_response`.
   *   ‚Ä¢ Mark `status = 'final'`.
   *   ‚Ä¢ NEVER create a second message.
   */
  const handleVoiceResponse = useCallback((text: string, _language: string) => {
    const clean = sanitizeText(text)
    const currentStreamId = streamingMsgIdRef.current
    streamingMsgIdRef.current = null // done streaming

    setMessages((prev) => {
      if (currentStreamId) {
        const idx = prev.findIndex((m) => m.id === currentStreamId)
        if (idx >= 0) {
          // Replace in-place ‚Äî same bubble, final text
          const updated = [...prev]
          updated[idx] = {
            ...updated[idx],
            text: clean,
            streaming: false,
            status: 'final' as const,
          }
          return updated
        }
      }
      // Edge case fallback: no streaming bubble existed (chunks were skipped).
      // Create one final message ‚Äî this is the ONLY path that may add a new message.
      return [
        ...prev,
        {
          id: `voice-ai-${Date.now()}`,
          text: clean,
          sender: 'agent' as const,
          timestamp: new Date(),
          streaming: true, // trigger typing animation for this edge case
          status: 'final' as const,
        },
      ]
    })
  }, [])

  /**
   * AUDIO_URL ‚Äî hold the URL and schedule delayed playback.
   * Enforce HTTPS in production (browsers block mixed content).
   */
  const handleVoiceAudioUrl = useCallback(
    (url: string, _language: string) => {
      const safeUrl =
        typeof window !== 'undefined' && window.location.protocol === 'https:'
          ? url.replace(/^http:\/\//i, 'https://')
          : url
      pendingAudioUrlRef.current = safeUrl
      scheduleAudioPlayback(safeUrl)
    },
    [scheduleAudioPlayback]
  )

  const handleVoiceError = useCallback((error: string) => {
    streamingMsgIdRef.current = null
    transcriptShownAtRef.current = null
    pendingAudioUrlRef.current = null

    const isPermission = error.includes('permission') || error.includes('denied')
    const displayText = isPermission
      ? '‚ö†Ô∏è Microphone permission denied. Please allow access and try again.'
      : `‚ö†Ô∏è Voice error: ${error}`
    setMessages((prev) => [
      ...prev,
      {
        id: Date.now().toString(),
        text: displayText,
        sender: 'agent',
        timestamp: new Date(),
      },
    ])
  }, [])

  // Voice recording integration
  const voiceRecorder = useVoiceRecorder({
    apiUrl: API_BASE,
    uiLanguage: currentLanguage,
    location: undefined,
    onTranscript: handleVoiceTranscript,
    onStreamChunk: handleVoiceStreamChunk,
    onResponse: handleVoiceResponse,
    onAudioUrl: handleVoiceAudioUrl,
    onError: handleVoiceError,
  })

  const scrollToBottom = useCallback(() => {
    if (isNearBottomRef.current) {
      messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' })
    }
  }, [])

  /** Check if user is near bottom of scroll (within 120px) */
  const handleScroll = useCallback(() => {
    const el = messagesContainerRef.current
    if (!el) return
    const threshold = 120
    isNearBottomRef.current =
      el.scrollHeight - el.scrollTop - el.clientHeight < threshold
  }, [])

  useEffect(() => {
    scrollToBottom()
  }, [messages, scrollToBottom])

  // ‚îÄ‚îÄ Viewport resize handler (mobile keyboard) ‚îÄ‚îÄ
  useEffect(() => {
    const vv = window.visualViewport
    if (!vv) return

    const handleResize = () => {
      // When mobile keyboard opens, visualViewport height shrinks.
      // Scroll to bottom to keep input visible.
      requestAnimationFrame(() => {
        scrollToBottom()
      })
    }

    vv.addEventListener('resize', handleResize)
    return () => vv.removeEventListener('resize', handleResize)
  }, [scrollToBottom])

  // Show welcome message on first load or when language changes
  useEffect(() => {
    const welcomeMessages: Record<Language, string> = {
      en: 'üëã Namaste! I\'m Krishi Setu, your AI farming companion. How can I help you today?',
      hi: 'üëã ‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§Æ‡•à‡§Ç ‡§ï‡•É‡§∑‡§ø ‡§∏‡•á‡§§‡•Å ‡§π‡•Ç‡§Å, ‡§Ü‡§™‡§ï‡§æ AI ‡§ï‡•É‡§∑‡§ø ‡§∏‡§π‡§æ‡§Ø‡§ï‡•§ ‡§Ü‡§ú ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§ï‡•à‡§∏‡•á ‡§Æ‡§¶‡§¶ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•Ç‡§Å?',
      bn: 'üëã ‡¶®‡¶Æ‡¶∏‡ßç‡¶ï‡¶æ‡¶∞! ‡¶Ü‡¶Æ‡¶ø ‡¶ï‡ßÉ‡¶∑‡¶ø ‡¶∏‡ßá‡¶§‡ßÅ, ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ AI ‡¶ï‡ßÉ‡¶∑‡¶ø ‡¶∏‡¶π‡¶æ‡¶Ø‡¶º‡¶ï‡•§ ‡¶Ü‡¶ú ‡¶Ü‡¶Æ‡¶ø ‡¶Ü‡¶™‡¶®‡¶æ‡¶ï‡ßá ‡¶ï‡ßÄ‡¶≠‡¶æ‡¶¨‡ßá ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶ø?',
      te: 'üëã ‡∞®‡∞Æ‡∞∏‡±ç‡∞ï‡∞æ‡∞∞‡∞Ç! ‡∞®‡±á‡∞®‡±Å ‡∞ï‡±É‡∞∑‡∞ø ‡∞∏‡±á‡∞§‡±Å, ‡∞Æ‡±Ä AI ‡∞µ‡±ç‡∞Ø‡∞µ‡∞∏‡∞æ‡∞Ø ‡∞∏‡∞π‡∞æ‡∞Ø‡∞ï‡±Å‡∞°‡∞ø‡∞®‡∞ø. ‡∞à‡∞∞‡±ã‡∞ú‡±Å ‡∞®‡±á‡∞®‡±Å ‡∞Æ‡±Ä‡∞ï‡±Å ‡∞é‡∞≤‡∞æ ‡∞∏‡∞π‡∞æ‡∞Ø‡∞Ç ‡∞ö‡±á‡∞Ø‡∞ó‡∞≤‡∞®‡±Å?',
      ta: 'üëã ‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç! ‡Æ®‡Ææ‡Æ©‡Øç ‡Æï‡Æø‡Æ∞‡ØÅ‡Æ∑‡Æø ‡Æö‡Øá‡Æ§‡ØÅ, ‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øç AI ‡Æµ‡Æø‡Æµ‡Æö‡Ææ‡ÆØ ‡Æâ‡Æ§‡Æµ‡Æø‡ÆØ‡Ææ‡Æ≥‡Æ∞‡Øç. ‡Æá‡Æ©‡Øç‡Æ±‡ØÅ ‡Æ®‡Ææ‡Æ©‡Øç ‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡Æé‡Æ™‡Øç‡Æ™‡Æü‡Æø ‡Æâ‡Æ§‡Æµ ‡ÆÆ‡ØÅ‡Æü‡Æø‡ÆØ‡ØÅ‡ÆÆ‡Øç?',
      mr: 'üëã ‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞! ‡§Æ‡•Ä ‡§ï‡•É‡§∑‡•Ä ‡§∏‡•á‡§§‡•Ç ‡§Ü‡§π‡•á, ‡§§‡•Å‡§Æ‡§ö‡§æ AI ‡§∂‡•á‡§§‡•Ä ‡§∏‡§π‡§æ‡§Ø‡•ç‡§Ø‡§ï. ‡§Ü‡§ú ‡§Æ‡•Ä ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§ï‡§∂‡•Ä ‡§Æ‡§¶‡§§ ‡§ï‡§∞‡•Ç ‡§∂‡§ï‡§§‡•ã?',
      gu: 'üëã ‡™®‡™Æ‡™∏‡´ç‡™§‡´á! ‡™π‡´Å‡™Ç ‡™ï‡´É‡™∑‡™ø ‡™∏‡´á‡™§‡´Å ‡™õ‡´Å‡™Ç, ‡™§‡™Æ‡™æ‡™∞‡´ã AI ‡™ñ‡´á‡™§‡´Ä ‡™∏‡™π‡™æ‡™Ø‡™ï. ‡™Ü‡™ú‡´á ‡™π‡´Å‡™Ç ‡™§‡™Æ‡™®‡´á ‡™ï‡´á‡™µ‡´Ä ‡™∞‡´Ä‡™§‡´á ‡™Æ‡™¶‡™¶ ‡™ï‡™∞‡´Ä ‡™∂‡™ï‡´Å‡™Ç?',
      kn: 'üëã ‡≤®‡≤Æ‡≤∏‡≥ç‡≤ï‡≤æ‡≤∞! ‡≤®‡≤æ‡≤®‡≥Å ‡≤ï‡≥É‡≤∑‡≤ø ‡≤∏‡≥á‡≤§‡≥Å, ‡≤®‡≤ø‡≤Æ‡≥ç‡≤Æ AI ‡≤ï‡≥É‡≤∑‡≤ø ‡≤∏‡≤π‡≤æ‡≤Ø‡≤ï. ‡≤á‡≤Ç‡≤¶‡≥Å ‡≤®‡≤æ‡≤®‡≥Å ‡≤®‡≤ø‡≤Æ‡≤ó‡≥Ü ‡≤π‡≥á‡≤ó‡≥Ü ‡≤∏‡≤π‡≤æ‡≤Ø ‡≤Æ‡≤æ‡≤°‡≤¨‡≤π‡≥Å‡≤¶‡≥Å?',
      ml: 'üëã ‡¥®‡¥Æ‡¥∏‡µç‡¥ï‡¥æ‡¥∞‡¥Ç! ‡¥û‡¥æ‡µª ‡¥ï‡µÉ‡¥∑‡¥ø ‡¥∏‡µá‡¥§‡µÅ ‡¥Ü‡¥£‡µç, ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ AI ‡¥ï‡¥æ‡µº‡¥∑‡¥ø‡¥ï ‡¥∏‡¥π‡¥æ‡¥Ø‡¥ø. ‡¥á‡¥®‡µç‡¥®‡µç ‡¥é‡¥®‡¥ø‡¥ï‡µç‡¥ï‡µç ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÜ ‡¥é‡¥ô‡µç‡¥ô‡¥®‡µÜ ‡¥∏‡¥π‡¥æ‡¥Ø‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡¥Ç?',
      pa: 'üëã ‡®∏‡®§ ‡®∏‡©ç‡®∞‡©Ä ‡®Ö‡®ï‡®æ‡®≤! ‡®Æ‡©à‡®Ç ‡®ï‡©ç‡®∞‡®ø‡®∏‡®º‡©Ä ‡®∏‡©á‡®§‡©Ç ‡®π‡®æ‡®Ç, ‡®§‡©Å‡®π‡®æ‡®°‡®æ AI ‡®ñ‡©á‡®§‡©Ä ‡®∏‡®π‡®æ‡®á‡®ï‡•§ ‡®Ö‡©±‡®ú ‡®Æ‡©à‡®Ç ‡®§‡©Å‡®π‡®æ‡®°‡©Ä ‡®ï‡®ø‡®µ‡©á‡®Ç ‡®Æ‡®¶‡®¶ ‡®ï‡®∞ ‡®∏‡®ï‡®¶‡®æ ‡®π‡®æ‡®Ç?',
    }

    // Replace existing welcome message or add first one
    setMessages((prev) => {
      const withoutWelcome = prev.filter((m) => m.id !== 'welcome')
      const welcome: Message = {
        id: 'welcome',
        text: welcomeMessages[currentLanguage],
        sender: 'agent',
        timestamp: new Date(),
      }
      return [welcome, ...withoutWelcome]
    })
  }, [currentLanguage])

  const handleSendMessage = async (text: string) => {
    const userMessage: Message = {
      id: Date.now().toString(),
      text,
      sender: 'user',
      timestamp: new Date(),
    }
    setMessages((prev) => [...prev, userMessage])
    setIsLoading(true)

    try {
      const location = await getUserLocation()

      const response = await apiClient.sendMessage({
        message: text,
        language: currentLanguage,
        location: location || undefined,
      })

      const agentMessage: Message = {
        id: (Date.now() + 1).toString(),
        text: sanitizeText(response.response),
        sender: 'agent',
        timestamp: new Date(),
        streaming: true,
        status: 'final',
      }
      setMessages((prev) => [...prev, agentMessage])
    } catch {
      const errorMessage: Message = {
        id: (Date.now() + 1).toString(),
        text: '‚ö†Ô∏è Sorry, I could not process your request. Please try again.',
        sender: 'agent',
        timestamp: new Date(),
      }
      setMessages((prev) => [...prev, errorMessage])
    } finally {
      setIsLoading(false)
    }
  }

  const handleVoiceButtonClick = async () => {
    try {
      if (voiceRecorder.state === 'idle') {
        // Reset refs for new voice interaction
        streamingMsgIdRef.current = null
        transcriptShownAtRef.current = null
        pendingAudioUrlRef.current = null
        clearAudioDelayTimer()

        await voiceRecorder.startRecording()
      } else if (voiceRecorder.state === 'recording') {
        await voiceRecorder.stopRecording()
      }
    } catch (err) {
      handleVoiceError(err instanceof Error ? err.message : String(err))
    }
  }

  const handleAudioPlayEnd = useCallback(() => {
    setAudioUrl(null)
    pendingAudioUrlRef.current = null
    transcriptShownAtRef.current = null
    voiceRecorder.markPlaybackDone()
  }, [voiceRecorder])

  // Interrupt audio when user starts new recording
  useEffect(() => {
    if (voiceRecorder.state === 'recording') {
      clearAudioDelayTimer()
      if (audioUrl) {
        setAudioUrl(null)
        pendingAudioUrlRef.current = null
      }
    }
  }, [voiceRecorder.state, audioUrl, clearAudioDelayTimer])

  // Cleanup audio delay timer on unmount
  useEffect(() => {
    return () => clearAudioDelayTimer()
  }, [clearAudioDelayTimer])

  // Determine if inputs should be disabled
  const isVoiceActive = voiceRecorder.state !== 'idle'
  const shouldDisableInput = isLoading || isVoiceActive

  return (
    <div className="flex-1 flex flex-col overflow-hidden relative min-h-0">
      {/* Background mesh */}
      <div className="absolute inset-0 bg-mesh pointer-events-none" />

      {/* Messages Container */}
      <div
        ref={messagesContainerRef}
        onScroll={handleScroll}
        className="relative flex-1 overflow-y-auto chat-scrollbar px-3 sm:px-4 pt-4 pb-2 min-h-0"
      >
        <div className="max-w-2xl mx-auto space-y-3 sm:space-y-4">
          {messages.map((message) => (
            <MemoizedMessageBubble key={message.id} message={message} />
          ))}
          {(isLoading || voiceRecorder.state === 'processing') && <LoadingStream />}
          {voiceRecorder.state === 'waiting_audio' && (
            <div className="flex justify-start animate-fade-in">
              <div className="text-xs text-gray-400 italic px-3 py-1 rounded-full bg-white/50 backdrop-blur-sm">
                Preparing audio‚Ä¶
              </div>
            </div>
          )}
          {/* Audio Player ‚Äî inline in message flow */}
          {audioUrl && (
            <div className="flex justify-start animate-slide-up">
              <AudioPlayer
                audioUrl={audioUrl}
                onPlayEnd={handleAudioPlayEnd}
                onError={() => {}}
              />
            </div>
          )}
          <div ref={messagesEndRef} className="h-1" />
        </div>
      </div>

      {/* Input Bar with integrated Voice Button */}
      <div className="relative flex-shrink-0 z-20 bg-sand-50/80 backdrop-blur-sm border-t border-gray-100/60 pb-safe">
        <InputBar
          onSendMessage={handleSendMessage}
          disabled={shouldDisableInput}
          language={currentLanguage}
          voiceButton={
            <VoiceButton
              state={voiceRecorder.state}
              onClick={handleVoiceButtonClick}
              disabled={isLoading}
            />
          }
        />
      </div>
    </div>
  )
}

/** Memoized wrapper ‚Äî only re-renders when message content actually changes */
const MemoizedMessageBubble = memo(MessageBubble, (prev, next) => {
  return (
    prev.message.id === next.message.id &&
    prev.message.text === next.message.text &&
    prev.message.streaming === next.message.streaming &&
    prev.message.status === next.message.status
  )
})
